<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">Using Visual Studio to develop and manage WildFly</title><link rel="alternate" href="https://www.mastertheboss.com/eclipse/jboss-tools/using-visual-studio-to-develop-and-manage-wildfly/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/eclipse/jboss-tools/using-visual-studio-to-develop-and-manage-wildfly/</id><updated>2023-05-22T09:13:15Z</updated><content type="html">Visual Studio Community Edition is completely free IDE for individual developers. Despite being free, it provides a rich set of features and capabilities comparable to the paid editions of Visual Studio. In this article we will learn how to use it to develop applications on top of WildFly application Server. Harness the Advantages of Visual ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How to use OpenShift Data Science for fraud detection</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/22/how-use-openshift-data-science-fraud-detection" /><author><name>Swati Kale</name></author><id>5502a392-3021-4849-9d81-77714a7af992</id><updated>2023-05-22T07:00:00Z</updated><published>2023-05-22T07:00:00Z</published><summary type="html">&lt;p&gt;The problem of detecting fraudulent transactions is intriguing for a data scientist. However, the overhead from setting up the technologies around it can be cumbersome. This is where &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-data-science"&gt;Red Hat OpenShift Data Science&lt;/a&gt;, along with &lt;a href="https://www.starburst.io/"&gt;Starburst&lt;/a&gt; and Intel &lt;a href="https://docs.openvino.ai/latest/home.html"&gt;OpenVino&lt;/a&gt;, come to the rescue. Now data scientists can focus on what they do best, model training and crafting; and OpenShift Data Science will do what we do best, providing the tools with the least overhead. &lt;/p&gt; &lt;p&gt;This article will cover detecting fraudulent transactions in a financial institution on Red Hat OpenShift Data Science.&lt;/p&gt; &lt;h2&gt;Workflow for credit card fraud detection&lt;/h2&gt; &lt;p&gt;Credit card fraud is a significant problem in the finance industry. Fraudsters can steal credit card information and use it to make unauthorized purchases. Fraud detection systems can monitor credit card transactions and identify suspicious activities, such as large transactions or transactions in unusual locations, and flag them for further investigation.&lt;/p&gt; &lt;p&gt;Building an effective fraud detection system using machine learning requires careful data collection, feature engineering, model selection, training and validation, deployment, monitoring, and updating to ensure that the system remains effective over time.&lt;/p&gt; &lt;p&gt;The diagram in Figure 1 shows the typical workflow for building and deploying machine learning models for detecting credit card payment fraud.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/workflow_1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/workflow_1.png?itok=zQ3ra5xb" width="512" height="57" alt="This diagram shows the typical workflow for building and deploying machine learning models for detecting credit card payment fraud." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: This diagram shows the typical workflow for building and deploying machine learning models for detecting financial fraud.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Overview of the fraud detection solution&lt;/h2&gt; &lt;p&gt;Figure 2 shows how to use the OpenShift Data Science platform to deploy an agile solution for detecting fraudulent credit card payment.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/solution.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/solution.png?itok=4TJY-qMF" width="512" height="310" alt="A diagram of the solution steps using the OpenShift Data Science platform to detect fraudulent credit card payment." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The solution steps using the OpenShift Data Science platform to detect fraudulent credit card payment.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The steps in the diagram are as follows:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;The data scientist uploads data to Amazon S3.&lt;/li&gt; &lt;li aria-level="1"&gt;Starburst Enterprise is connected to Amazon S3.&lt;/li&gt; &lt;li aria-level="1"&gt;The data scientist uses the query editor in Starburst to preprocess the data and visualize the dataset.&lt;/li&gt; &lt;li aria-level="1"&gt;The cleaned data is uploaded back to Amazon S3 via Starburst.&lt;/li&gt; &lt;li aria-level="1"&gt;Next, the data scientist creates a data science project within OpenShift Data Science which enables them to launch JupyterLab along with specific dependencies.&lt;/li&gt; &lt;li aria-level="1"&gt;Retrieves the cleaned data from Amazon S3. Using the data, they train machine learning models and upload them back to Amazon S3.&lt;/li&gt; &lt;li aria-level="1"&gt;The trained models are then served by the OpenVINO Model Server.&lt;/li&gt; &lt;li aria-level="1"&gt;Finally, the models are deployed for fraud detection.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;What is Red Hat OpenShift Data Science?&lt;/h2&gt; &lt;p&gt;Red Hat OpenShift Data Science is a platform for developing, deploying, and managing machine learning workflows and models in a containerized environment. It is built on top of the Red Hat OpenShift Container Platform and provides a suite of tools and services for ML workflows, including data preparation, model training, and model deployment. Read more about OpenShift Data Science in the article, &lt;a href="https://developers.redhat.com/blog/2021/04/27/4-reasons-youll-love-using-red-hat-openshift-data-science"&gt;4 reasons you’ll love using OpenShift Data Science&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;OpenShift Data Science is fully integrated with AI/ML tools, including &lt;a href="https://blog.jupyter.org/jupyterlab-is-ready-for-users-5a6f039b8906"&gt;JupyterLab&lt;/a&gt; with predefined notebook images for launching notebooks with access to core AI/ML libraries and frameworks like &lt;a href="https://www.tensorflow.org/"&gt;TensorFlow&lt;/a&gt;. It provides a single interface for managing and implementing all ML steps, including model deployment and training, and is backed by local storage for saving tasks for later use.&lt;/p&gt; &lt;p&gt;In our workflow, we created a data connection to Amazon S3 for adding data to the project. OpenShift Data Science is also integrated with Kserve ModelMesh Serving, which provides out-of-the-box integration with model servers and allows selecting the model server and data connection. The user can view the current status of deployed models and their inference endpoints.&lt;/p&gt; &lt;p&gt;We configured the OpenVINO Model Server, which allows for easy deployment and management of pre-trained deep learning models in production environments. It provides a flexible and scalable platform for deploying deep learning models with RESTful APIs, making it easy to integrate the models with applications.&lt;/p&gt; &lt;h2&gt;Why we use Starburst and Amazon S3&lt;/h2&gt; &lt;p&gt;Starburst Enterprise provides a powerful, user-friendly interface for managing Trino clusters, monitoring query performance, and identifying bottlenecks. It is a popular tool for organizations that use Trino for distributed SQL query processing and require a comprehensive interface for managing their Trino clusters. It unlocks access to data where it lives, no data movement required, giving your teams fast and accurate access to more data for analysis using query editor.&lt;/p&gt; &lt;p&gt;We use Amazon S3 for storing the datasets and trained models. The user can make use of the Amazon S3 in data collection, preprocessing data, and model training phase.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;OpenShift Data Science sandbox&lt;/li&gt; &lt;li aria-level="1"&gt;Starburst Enterprise license&lt;/li&gt; &lt;li aria-level="1"&gt;Starburst Operator installed&lt;/li&gt; &lt;li aria-level="1"&gt;Read and write access to Amazon S3 bucket&lt;/li&gt; &lt;li aria-level="1"&gt;Access to the original dataset&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;The environment setup&lt;/h2&gt; &lt;p&gt;You can find guidance for setting up the environment by following these links:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science/download"&gt;Set up your Red Hat OpenShift Data Science environment&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;If you haven’t already, you can find information about getting an instance of Red Hat OpenShift Data Science on the &lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science/download"&gt;developer page&lt;/a&gt;. You can spin up your own account on the free OpenShift Data Science Sandbox or learn about installing on your OpenShift cluster.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/RHEcosystemAppEng/rhods-fraud-detection/blob/master/Starburst.md"&gt;Set up Starburst operator on OpenShift Data Science&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/RHEcosystemAppEng/rhods-fraud-detection/blob/master/Starburst.md"&gt;Connect Starburst to Amazon S3 and launch Starburst Query Editor&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/RHEcosystemAppEng/rhods-fraud-detection"&gt;Try out the fraud detection use case on OpenShift Data Science&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Watch the demo video featuring &lt;strong&gt;Suvro Gosh&lt;/strong&gt; (creator) and &lt;strong&gt;Swati Kale&lt;/strong&gt;:&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h2&gt;OpenShift Data Science simplifies fraud detection&lt;/h2&gt; &lt;p&gt;Red Hat OpenShift Data Science provides a fully supported environment in which to rapidly develop, train, and test AI/ML models in the public cloud before deploying in production. This use case can be extended by bringing your algorithm for fraud detection and model framework using the OpenShift Data Science ecosystem. Feel free to comment below if you have questions. We welcome your feedback!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/22/how-use-openshift-data-science-fraud-detection" title="How to use OpenShift Data Science for fraud detection"&gt;How to use OpenShift Data Science for fraud detection&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Swati Kale</dc:creator><dc:date>2023-05-22T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.38.0 released!</title><link rel="alternate" href="https://blog.kie.org/2023/05/kogito-1-38-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2023/05/kogito-1-38-0-released.html</id><updated>2023-05-22T05:42:46Z</updated><content type="html">We are glad to announce that the Kogito 1.38.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Data Index addons that allow running indexing capabilities as part of Kogito runtimes, incorporate now the interaction with the Job Service embedded Quarkus extension. * Rocksdb persistence quarkus add-on * Serverless Workflow embedded executor dependency list has been substantially reduced. * Added support for full GVK to Knative custom function * Added support for existing “quarkus.flyway.locations” values  For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.28.0 artifacts are available at the . A detailed changelog for 1.38.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title>How to use RHEL application compatibility guidelines</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/18/how-use-rhel-application-compatibility-guidelines" /><author><name>Carlos O'Donell</name></author><id>6da035d9-239a-4926-aeaa-77409c970e44</id><updated>2023-05-18T07:00:00Z</updated><published>2023-05-18T07:00:00Z</published><summary type="html">&lt;p&gt;In this three-part series, we explore the &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; published application compatibility guidelines (ACG), and how developers can use them to ensure their application remains compatible with future releases of RHEL. Building applications can be difficult, and building applications that continue to operate after an in-place distribution upgrade is even harder. How does Red Hat Enterprise Linux make it easier? It provides guidelines and guarantees that you can follow to improve application compatibility.&lt;/p&gt; &lt;p&gt;In this article, we will expand on the concept of application compatibility. In the second part, we will review the topic of compatibility with more examples. In the third article, we will discuss container userspace compatibility with the host kernel services.&lt;/p&gt; &lt;h2&gt;What is application compatibility?&lt;/h2&gt; &lt;p&gt;What we call application compatibility is traditionally referred to as backwards compatibility. It is the ability to run an unmodified application binary on the current or newer version of the distribution and have it operate correctly (i.e., compatible with the release of the distribution).&lt;/p&gt; &lt;h3&gt;Maintaining compatibility&lt;/h3&gt; &lt;p&gt;Application compatibility is maintained by ensuring that the dependencies of the application continue to be provided and that the application continues to function as intended.&lt;/p&gt; &lt;p&gt;When we talk about &lt;a href="https://developers.redhat.com/topics/c"&gt;C or C++&lt;/a&gt; applications, this could mean that the libraries the application needs are still present and providing the expected set of features and behaviors. When we talk about &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;, it means continuing to provide the modules the Python script requires or the language features it needs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/18/whats-new-red-hat-enterprise-linux-9"&gt;Red Hat Enterprise Linux 9&lt;/a&gt; was released in May of 2022. At the same time, the &lt;a href="https://access.redhat.com/articles/rhel9-abi-compatibility"&gt;Red Hat Enterprise Linux 9: Application Compatibility Guide&lt;/a&gt; (RHEL ACG) was published, and the &lt;a href="https://access.redhat.com/support/policy/rhel-container-compatibility"&gt;Red Hat Enterprise Linux Container Compatibility Matrix&lt;/a&gt; (RHEL CCM) was updated. These two guides are key to helping developers learn about the guidelines they can follow to ensure application compatibility with a given RHEL release.&lt;/p&gt; &lt;p&gt;In the first two parts of this series, we will focus on the RHEL ACG.&lt;/p&gt; &lt;h2&gt;Defining API and ABI&lt;/h2&gt; &lt;p&gt;This is a quick recap of the detailed definitions in the compatibility guide. An API is the application programming interface, and it represents the set of conventions, features, or behaviors at compile time. An ABI is the application binary interface, and it represents the set of conventions, features, or behaviors at run time.&lt;/p&gt; &lt;p&gt;An API can be a source level function call (e.g.,&lt;code&gt;exit(0)&lt;/code&gt;). An ABI can be the actual implementation of the &lt;code&gt;void exit(int status)&lt;/code&gt; function in the C library.&lt;/p&gt; &lt;h2&gt;Components of the application compatibility guide&lt;/h2&gt; &lt;p&gt;The ACG is split into two major sections:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Guidelines for preserving application compatibility across minor and major RHEL versions.&lt;/li&gt; &lt;li aria-level="1"&gt;The binary rpm package list and the compatibility guarantees.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The distribution places the binary rpm packages into one of four compatibility levels. It can be viewed as a narrowing set of compatibility guarantees across minor and major upgrade paths.&lt;/p&gt; &lt;div&gt; &lt;table border="1" cellspacing="0" width="672"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;⇓ Compatibility Level / RHEL Version ⇒&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;RHEL X.Y&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;RHEL X.Y+1&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;RHEL X+1&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;RHEL X+2&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;1&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;API and ABI compatible&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;API and ABI compatible&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;ABI compatible&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;ABI compatible&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;2&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;API and ABI compatible&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;API and ABI compatible&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;3&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;API and ABI defined by life cycle&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;4&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;API and ABI subject to change at any time&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt; &lt;p&gt;Following the guidelines and using only packages that provide the guarantees your application needs will ensure that your application remains as compatible as possible across minor and major RHEL version upgrades.&lt;/p&gt; &lt;p&gt;To be compatible with minor version upgrades, it requires you to use only packages in compatibility level 2 or level 1, with review of packages used in level 3. To be compatible with major version upgrades, it requires you to use only packages in compatibility level 1, with review of packages used in level 3.&lt;/p&gt; &lt;p&gt;The default for packages in RHEL is compatibility level 2, which ensures that applications keep working across minor version upgrades.&lt;/p&gt; &lt;h2&gt;Workloads, services, containers, and packages&lt;/h2&gt; &lt;p&gt;You’ll notice that we talk a lot about packages. We talk about packages because it allows developers to decide which parts of decomposed workloads will be compatible with RHEL for a long time and which parts you might have to recompile, rewrite, or forward port.&lt;/p&gt; &lt;p&gt;At the highest level, you are going to have a workload that you want to support over time. That workload does something useful. Workloads can be managed with &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; like &lt;a href="https://www.ansible.com/"&gt;Ansible&lt;/a&gt; or &lt;a href="https://backstage.io/"&gt;Backstage&lt;/a&gt;. I am not going to talk about workloads because as an abstraction, they are useful for talking at a very high level. When you think about it, the workload of “transactional request processing system” is too abstract for us to talk about compatibility.&lt;/p&gt; &lt;p&gt;A workload can be decomposed into services, and at this point it starts getting closer to the level at which we are talking about cross-RHEL-release compatibility guidelines. Services have concrete instantiations like an AMQP (Advanced Message Queuing Protocol) server running locally that handles messages (e.g., &lt;a href="https://www.rabbitmq.com/"&gt;RabbitMQ&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;When it comes to the smallest installable unit of something on RHEL, we can talk generally about containers or rpm packages. I’m going to defer the conversation about containers until part 3 of this series since the compatibility of containers is covered in the &lt;a href="https://access.redhat.com/support/policy/rhel-container-compatibility"&gt;Red Hat Enterprise Linux Container Compatibility Matrix&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;So either we are talking about the rpm packages in the container, or we’re talking about rpm packages on the host. As a developer you are still responsible for the decomposition of the workload and services into packages (software collections or modules are still delivered as packages). If packages change between major version upgrades then &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/upgrading_from_rhel_8_to_rhel_9/planning-an-upgrade_upgrading-from-rhel-8-to-rhel-9"&gt;RHEL Leapp&lt;/a&gt; is there to help ensure the same features are present even if the package changes names.&lt;/p&gt; &lt;h2&gt;Example: A RHEL 7 application written in C&lt;/h2&gt; &lt;p&gt;Explaining application compatibility guidelines is easier with an example. Say you are building an application in C that you started developing on RHEL 7, and you are now looking at RHEL 8 and RHEL 9 for eventual deployment.&lt;/p&gt; &lt;p&gt;Let’s dive in by using a simple C “Hello World” example and see what the application compatibility guidelines say about each of the development steps in building the application.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;#include &lt;stdio.h&gt; int main (void) { printf ("Hello World!\n"); return 0; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Compile, inspect, and run as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ gcc -o helloworld helloworld.c $ ldd helloworld linux-vdso.so.1 =&gt; (0x00007ffd09d75000) libc.so.6 =&gt; /lib64/libc.so.6 (0x00007fe51a633000) /lib64/ld-linux-x86-64.so.2 (0x00007fe51aa01000) $ readelf -W --dyn-syms helloworld Symbol table '.dynsym' contains 4 entries: Num: Value Size Type Bind Vis Ndx Name 0: 0000000000000000 0 NOTYPE LOCAL DEFAULT UND 1: 0000000000000000 0 FUNC GLOBAL DEFAULT UND puts@GLIBC_2.2.5 (2) 2: 0000000000000000 0 FUNC GLOBAL DEFAULT UND __libc_start_main@GLIBC_2.2.5 (2) 3: 0000000000000000 0 NOTYPE WEAK DEFAULT UND __gmon_start__ $./helloworld Hello World!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There are several important takeaways from this example such as:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Links only the libraries it needs (ClLibrary: glibc, implicitly). &lt;ul&gt;&lt;li aria-level="2"&gt;Improves compatibility with future RHEL versions by limiting library use.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Uses C library header files for the APIs it uses (i.e., &lt;strong&gt;#include&lt;/strong&gt; directive). &lt;ul&gt;&lt;li aria-level="2"&gt;Ensures the development packages are installed provide those files and ensures any compatibility mechanisms are active.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Application is executed in an environment that is as new as the system it was compiled on. &lt;ul&gt;&lt;li aria-level="2"&gt;Backwards compatibility is guaranteed.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Does not use static linking. &lt;ul&gt;&lt;li aria-level="2"&gt;Robust runtime behavior of statically linked applications requires that the runtime environment match exactly the build time environment.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Avoids explicit dependency on a given Linux kernel version. &lt;ul&gt;&lt;li aria-level="2"&gt;Improves compatibility if the application is later packaged as a container.&lt;/li&gt; &lt;li aria-level="2"&gt;Improves compatibility with future RHEL kernel versions.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The &lt;a href="https://access.redhat.com/articles/rhel-abi-compatibility"&gt;RHEL7 Application Compatibility Guide&lt;/a&gt; recommends all these points and more.&lt;/p&gt; &lt;p&gt;So you might be asking, “Great, but what does that mean for my RHEL 8 and RHEL 9 migration?” Let's look first at the dependencies of the application. In this case, it’s only two dynamic symbols in the rpm package glibc, i.e. &lt;em&gt;&lt;strong&gt;puts@GLIBC_2.2.5&lt;/strong&gt;&lt;/em&gt; and &lt;em&gt;&lt;strong&gt;__libc_start_main@GLIBC_2.2.5&lt;/strong&gt;&lt;/em&gt;. Tracking which package provides these symbols is done by rpm and dynamic shared object names, and symbol set provides. It is a topic for another article, so for now, we'll skip this part.&lt;/p&gt; &lt;p&gt;The glibc binary rpm package is one of a small set of packages that is in compatibility level 1, and these are very important packages that are guaranteed to have a compatible ABI for at least three major RHEL releases. That means that the application we just created in this example should run correctly in RHEL 7, RHEL 8, and RHEL 9. Let's try it out.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;On RHEL 7:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ uname -r 3.10.0-1160.88.1.el7.x86_64 $ sha1sum helloworld beb000e63f609b09583a719ece01ea58b50dd2f8 helloworld $./helloworld Hello World!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;On RHEL 8:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ uname -r 4.18.0-468.el8.x86_64 $ sha1sum helloworld beb000e63f609b09583a719ece01ea58b50dd2f8 helloworld $./helloworld Hello World!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;On RHEL 9:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ uname -r 5.14.0-162.21.1.el9_1.x86_64 $ sha1sum helloworld beb000e63f609b09583a719ece01ea58b50dd2f8 helloworld $./helloworld Hello World! &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Having an application that only depends on compatibility level 1 packages allows the binary to run across three major RHEL releases without any issues. By 2024, that will be ten years of runtime compatibility!&lt;/p&gt; &lt;h2&gt;Example: A RHEL 7 application written in C using OpenSSL&lt;/h2&gt; &lt;p&gt;Let us take that example a bit further and try to use a library like OpenSSL that is only compatibility level 2, which means the ABI guarantee is only for the minor version upgrades of RHEL. That means that if you compile on RHEL 7.0, the application should still be working in RHEL 7.9 (last y-stream release), but is not guaranteed to work in RHEL 8.0 without recompilation in that distribution.&lt;/p&gt; &lt;p&gt;The example program here uses OpenSSL’s BIO interface to read from standard input and write the same thing to standard output. While this doesn’t exercise all of OpenSSL’s features, it does help us illustrate application compatibility.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;#include &lt;openssl/ssl.h&gt; #include &lt;openssl/bio.h&gt; #include &lt;stdlib.h&gt; #define BUFSIZE 4096 int main(void) { char buf[BUFSIZE]; int bin, bout; BIO *bio_stdin, *bio_stdout; bio_stdin = BIO_new_fp(stdin, BIO_NOCLOSE); bio_stdout = BIO_new_fp(stdout, BIO_NOCLOSE); if (bio_stdin == NULL || bio_stdout == NULL) exit (1); while ((bin = BIO_read (bio_stdin, buf, BUFSIZE)) &gt; 0) { bout = BIO_write (bio_stdout, buf, bin); if (bin != bout) exit (1); } BIO_free (bio_stdout); BIO_free (bio_stdin); return 0; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Compiled, inspected, and run like the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ gcc -o bio-cp bio-cp.c -lssl -lcrypto $ ldd./bio-cp linux-vdso.so.1 =&gt; (0x00007ffc4cfa1000) libssl.so.10 =&gt; /lib64/libssl.so.10 (0x00007f1811c65000) libcrypto.so.10 =&gt; /lib64/libcrypto.so.10 (0x00007f1811802000) libc.so.6 =&gt; /lib64/libc.so.6 (0x00007f1811434000) libgssapi_krb5.so.2 =&gt; /lib64/libgssapi_krb5.so.2 (0x00007f18111e7000) libkrb5.so.3 =&gt; /lib64/libkrb5.so.3 (0x00007f1810efe000) libcom_err.so.2 =&gt; /lib64/libcom_err.so.2 (0x00007f1810cfa000) libk5crypto.so.3 =&gt; /lib64/libk5crypto.so.3 (0x00007f1810ac7000) libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007f18108c3000) libz.so.1 =&gt; /lib64/libz.so.1 (0x00007f18106ad000) /lib64/ld-linux-x86-64.so.2 (0x00007f1811ed7000) libkrb5support.so.0 =&gt; /lib64/libkrb5support.so.0 (0x00007f181049d000) libkeyutils.so.1 =&gt; /lib64/libkeyutils.so.1 (0x00007f1810299000) libresolv.so.2 =&gt; /lib64/libresolv.so.2 (0x00007f181007f000) libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00007f180fe63000) libselinux.so.1 =&gt; /lib64/libselinux.so.1 (0x00007f180fc3c000) libpcre.so.1 =&gt; /lib64/libpcre.so.1 (0x00007f180f9da000) $ readelf -W --dyn-syms bio-cp Symbol table '.dynsym' contains 15 entries: Num: Value Size Type Bind Vis Ndx Name 0: 0000000000000000 0 NOTYPE LOCAL DEFAULT UND 1: 0000000000000000 0 FUNC GLOBAL DEFAULT UND BIO_write@libcrypto.so.10 (3) 2: 0000000000000000 0 FUNC GLOBAL DEFAULT UND BIO_read@libcrypto.so.10 (3) 3: 0000000000000000 0 FUNC GLOBAL DEFAULT UND BIO_free@libcrypto.so.10 (3) 4: 0000000000000000 0 FUNC GLOBAL DEFAULT UND exit@GLIBC_2.2.5 (2) 5: 0000000000000000 0 FUNC GLOBAL DEFAULT UND BIO_new_fp@libcrypto.so.10 (3) 6: 0000000000000000 0 FUNC GLOBAL DEFAULT UND __libc_start_main@GLIBC_2.2.5 (2) 7: 0000000000000000 0 NOTYPE WEAK DEFAULT UND __gmon_start__ 8: 0000000000601060 8 OBJECT GLOBAL DEFAULT 25 stdout@GLIBC_2.2.5 (2) 9: 0000000000601054 0 NOTYPE GLOBAL DEFAULT 24 _edata 10: 0000000000601078 0 NOTYPE GLOBAL DEFAULT 25 _end 11: 0000000000601068 8 OBJECT GLOBAL DEFAULT 25 stdin@GLIBC_2.2.5 (2) 12: 0000000000400640 0 FUNC GLOBAL DEFAULT 11 _init 13: 0000000000601054 0 NOTYPE GLOBAL DEFAULT 25 __bss_start 14: 0000000000400914 0 FUNC GLOBAL DEFAULT 14 _fini $./bio-cp &lt; helloworld.c #include &lt;stdio.h&gt; int main (void) { printf ("Hello World!\n"); return 0; } $ echo $? 0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The application compatibility guidelines for RHEL 7 state that to be compatible, you must recompile at each major release to ensure ABI compatibility. Again, this is because OpenSSL is in compatibility level 2. Let's put this to the test.&lt;/p&gt; &lt;p&gt;On RHEL 8:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$./bio-cp ./bio-cp: error while loading shared libraries: libssl.so.10: cannot open shared object file: No such file or directory&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On RHEL 9:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$./bio-cp ./bio-cp: error while loading shared libraries: libssl.so.10: cannot open shared object file: No such file or directory&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The default version of OpenSSL in RHEL7 is 1.0. In RHEL8 it is 1.1.1, and in RHEL9 it is 3.0. Each version is unique, and to use them requires the application to be compiled against the specific version in the distribution.&lt;/p&gt; &lt;p&gt;The application compatibility guide lists OpenSSL as compatibility level 2, and it bears repeating that such libraries have guaranteed ABI compatibility only within the major version of RHEL in which they were released.&lt;/p&gt; &lt;p&gt;The recent &lt;a href="https://www.redhat.com/en/blog/experience-bringing-openssl-30-rhel-and-fedora"&gt;migration of RHEL9 to OpenSSL 3.0&lt;/a&gt; was a technically complex migration. Red Hat went above and beyond by providing customers with compatibility packages to facilitate application developers. These compatibility packages provide the libraries required to meet the ABI requirements of OpenSSL using applications. Similar packages were also provided in RHEL 8 to enable the migration from OpenSSL 1.0 to 1.1.1. Lets try out the compatibility packages.&lt;/p&gt; &lt;p&gt;On RHEL 8 with compat-openssl10:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ dnf install compat-openssl10 ... $ ldd bio-cp linux-vdso.so.1 (0x00007fff5df5b000) libssl.so.10 =&gt; /lib64/libssl.so.10 (0x00007fac6d7ed000) libcrypto.so.10 =&gt; /lib64/libcrypto.so.10 (0x00007fac6d38b000) libc.so.6 =&gt; /lib64/libc.so.6 (0x00007fac6cfc6000) libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007fac6cdc2000) libz.so.1 =&gt; /lib64/libz.so.1 (0x00007fac6cbaa000) /lib64/ld-linux-x86-64.so.2 (0x00007fac6da5c000) $./bio-cp &lt; helloworld.c #include &lt;stdio.h&gt; int main (void) { printf ("Hello World!\n"); return 0; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;While the ACG says not to rely on such compatibility for compatibility level 2 packages, there are cases like this one with OpenSSL where Red Hat has gone above and beyond to ensure customer success.&lt;/p&gt; &lt;h2&gt;Learn more about RHEL ACG&lt;/h2&gt; &lt;p&gt;We have discussed application compatibility, and how Red Hat provides guidelines and guarantees to help developers create applications that are compatible with minor and major version upgrades for Red Hat Enterprise Linux. We have looked at the first important document that provides those guidelines for RHEL 9, the &lt;a href="https://access.redhat.com/articles/rhel9-abi-compatibility"&gt;Red Hat Enterprise Linux 9: Application Compatibility Guide&lt;/a&gt; (RHEL ACG). We have looked at two examples that showcase application compatibility across major version upgrades.&lt;/p&gt; &lt;p&gt;I encourage all developers out there to read the &lt;a href="https://access.redhat.com/articles/rhel9-abi-compatibility"&gt;Red Hat Enterprise Linux 9: Application Compatibility Guide&lt;/a&gt; (RHEL ACG), and make sure you are following the best practice guidelines to meet the compatibility needs of your application, service, or workload. Stay tuned for part 2 of this series where we will look again at the RHEL ACG, but dive into more complex examples. Part 3 will cover container compatibility and describe the &lt;a href="https://access.redhat.com/support/policy/rhel-container-compatibility"&gt;Red Hat Enterprise Linux Container Compatibility Matrix&lt;/a&gt; (RHEL CCM).&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/18/how-use-rhel-application-compatibility-guidelines" title="How to use RHEL application compatibility guidelines"&gt;How to use RHEL application compatibility guidelines&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Carlos O'Donell</dc:creator><dc:date>2023-05-18T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - May, 15 2023</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2023-05-18.html" /><category term="quarkus" /><category term="java" /><category term="wildfly" /><category term="camel" /><category term="strimzi" /><category term="podman" /><author><name>Francesco Marchioni</name><uri>https://www.jboss.org/people/francesco-marchioni</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2023-05-18.html</id><updated>2023-05-18T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, java, wildfly, camel, strimzi, podman"&gt; &lt;h1&gt;This Week in JBoss - May, 15 2023&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Happy Friday, everyone!&lt;/p&gt; &lt;p&gt;Here is another edition of the JBoss Editorial with exciting news and updates from your JBoss communities.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Here are the most recent releases for this edition:&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-3-0-3-final-released/"&gt;Quarkus 3.0.3.Final released&lt;/a&gt; - The Quarkus Team released Quarkus 3.0.3.Final, as part of the second maintenance release of our 3.0 release train. This release contains bugfixes and documentation improvements.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2023/05/kogito-1-37-0-released.html"&gt;Kogito 1.37 released&lt;/a&gt; - The new Kogito release features enhancements in the flow actions, workflow definitions logging and service discovery.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://camel.apache.org/releases/release-4.0.0-M3/"&gt;Camel RELEASE 4.0.0-M3 available&lt;/a&gt; - The Camel community announces the availability of Camel 4.0.0-M3, the third milestone towards a new 4.0.0 major release which comes with 155 new features and improvements.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_exactly_once_semantics_with_kafka_transactions"&gt;Exactly-once semantics with Kafka transactions&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://strimzi.io/blog/2023/05/03/kafka-transactions/"&gt;Exactly-once semantics with Kafka transactions&lt;/a&gt;, by Federico Valeri.&lt;/p&gt; &lt;p&gt;Kafka transactions play a vital role in guaranteeing the reliability and integrity of data, making them a pivotal component of the Kafka platform. Nevertheless, these benefits are accompanied by a trade-off in terms of decreased throughput and added latency, necessitating potential adjustments. Neglecting to monitor transactions that remain unresolved can adversely affect the availability of the service. This article sheds some light on this topic.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_how_to_use_the_new_openshift_quick_starts_to_deploy_jboss_eap"&gt;How to use the new OpenShift quick starts to deploy JBoss EAP&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2023/05/15/how-use-new-openshift-quick-starts-deploy-jboss-eap"&gt;How to use the new OpenShift quick starts to deploy JBoss EAP&lt;/a&gt;, by Philip Hayes&lt;/p&gt; &lt;p&gt;This article showcases the latest JBoss EAP quick start, specifically created to assist developers who are already familiar with conventional JBoss EAP deployments. Its purpose is to provide a comprehensive walkthrough on constructing and deploying application images on OpenShift. The quick start offers valuable guidance on utilizing Helm to generate the necessary build configs, deployment configs, and external routes for building and deploying JBoss EAP applications on OpenShift.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_integrate_excel_with_drools_on_openshift_with_knative_and_quarkus"&gt;Integrate Excel with Drools on OpenShift with Knative and Quarkus&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2023/05/integrate-excel-with-drools-on-openshift-with-knative-and-quarkus.html"&gt;Integrate Excel with Drools on OpenShift with Knative and Quarkus&lt;/a&gt;, by Matteo Mortari&lt;/p&gt; &lt;p&gt;In this blog post, Matteo shares the results of a technical exploration of bringing together different technologies and platforms. At the end of the day, he combined things like regular spreadsheet applications (like Excel), serverless platforms (Knative on OpenShift), and our rule engine Drools to see how they could work together.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_podman_desktop_beginners_guide"&gt;Podman Desktop Beginner’s Guide&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://www.mastertheboss.com/soa-cloud/docker/podman-desktop-a-beginners-guide-to-containerization/"&gt;Podman Desktop Guide&lt;/a&gt;, by Francesco Marchioni&lt;/p&gt; &lt;p&gt;In this tutorial, I’m introducing the Podman desktop UI which simplifies the usage and management of container images. As an example, we will learn how to pull, start, and monitor a WildFly Container image with just a few clicks!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_whats_new_in_red_hats_migration_toolkit_for_applications_6_1"&gt;What’s new in Red Hat’s migration toolkit for applications 6.1&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2023/05/10/whats-new-red-hats-migration-toolkit-applications-61"&gt;What’s new in Red Hat’s migration toolkit for applications 6.1&lt;/a&gt;, by Yashwanth Maheshwaram&lt;/p&gt; &lt;p&gt;Red Hat Migration Toolkit is an essential tool to simplify the upgrade or migration of a large set of enterprise applications. This article gives you an update with the latest news and includes a great demo video.&lt;/p&gt; &lt;p&gt;&lt;em&gt;That’s all folks! Please join us again in two weeks for another round of our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/francesco-marchioni.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Francesco Marchioni&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Francesco Marchioni</dc:creator></entry><entry><title type="html">Toward a reliable and fully recoverable Drools stateful session</title><link rel="alternate" href="https://blog.kie.org/2023/05/toward-a-reliable-and-fully-recoverable-drools-stateful-session.html" /><author><name>Mario Fusco</name></author><id>https://blog.kie.org/2023/05/toward-a-reliable-and-fully-recoverable-drools-stateful-session.html</id><updated>2023-05-17T14:09:35Z</updated><content type="html">One of the most important features of Drools is the possibility of evaluating rules in an interactive and conversational way, allowing to use inference to make iterative changes to facts over time and preserving the state of a session among different invocations. Actually this characteristic is so widely used, for instance when performing complex event processing, to be the default behavior of a .  The state of a session is of course made of many different data structures that live on the JVM heap. This implies that its lifespan is bound to the one of the hosting JVM and gets lost if for any reason the JVM itself, or more likely the machine, physical or virtual, on which that JVM is running, experiences a failure and is terminated.  This can be a serious limitation in the use of a stateful session, especially in a cloud environment where also the node running the session has to be considered ephemeral and can be killed at any moment. In this blog post we will introduce the implementation of a Drools reliable session that is capable of automatically persisting its internal state while it is running thus allowing it to recreate a new session from that persisted state when for any reason the original session becomes unavailable. INTRODUCING DROOLS RELIABLE SESSION The version 8.38.0.Final of Drools introduces a containing a first implementation of the Drools reliable session. At the moment it is made of 2 submodules. The first one defines the common API and general implementation of the reliable session, abstracting away the persistence layer in order to make it possible to plug different concrete implementations of this layer. The second provides one of such persistence layer implementations based on , while other modules relying on different technologies for persistence will be made available in future. Through this module it is possible to create a reliable KieSession as it follows KieSessionConfiguration conf = KieServices.get().newKieSessionConfiguration(); conf.setOption(PersistedSessionOption.newSession(PersistedSessionOption.Strategy.STORES_ONLY)); KieSession ksession = kbase.newKieSession(conf, null); and start using it as any other normal Drools stateful session. The only requirement to resume this session after a JVM crash is keeping its identifier on the client side long savedSessionId = session.getIdentifier(); so that it will be possible at a later time to recreate a new stateful session preserving the same state of the old one. KieSessionConfiguration conf = KieServices.get().newKieSessionConfiguration(); conf.PersistedSessionOption.fromSession(savedSessionId, PersistedSessionOption.Strategy.STORES_ONLY)); KieSession ksession = kbase.newKieSession(conf, null); PUTTING THE RELIABLE SESSION AT WORK IN THE CLOUD As anticipated, this new capability for a Drools session of being reliable makes it a perfect fit for a cloud environment where a node hosting a long running stateful computation could suddenly die for many different reasons. When this happens another node can be started without losing any data of the old session and keeping using it as nothing happened. To demonstrate how this works we developed a based on implementing a simple rule service to validate a set of loan applications and get the total amount of approved loans in a reliable stateful session. In this way the state of a session is persisted across different executions, so if the server is shut down and then restarted the state of an old session is not lost. This demo persists the state of a session using Infinispan running in embedded mode. This demo application can be also . While doing so you can try to scale to zero the deployment running this application. If you do so, of course the application won’t respond anymore to any further REST call, but scaling it up to one again the reliable session will automatically restore its state and the computation can continue as nothing happened. A video demonstrating how this works is available here: The post appeared first on .</content><dc:creator>Mario Fusco</dc:creator></entry><entry><title>Build lean Node.js container images with UBI and Podman</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/17/build-lean-nodejs-container-images-ubi-and-podman" /><author><name>Evan Shortiss</name></author><id>e02331c3-e1ff-4cd5-b43f-8ebc71e5f23f</id><updated>2023-05-17T07:00:00Z</updated><published>2023-05-17T07:00:00Z</published><summary type="html">&lt;p&gt;Building a &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; image for your &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; application might sound like a trivial task, but there are some pitfalls you’ll want to avoid along the path to containerizing your application. It is all too easy to accidentally include sensitive files, run more than one process in the container, run as root, or ship bloated container images. Because all of these mistakes reflect an image constructed without due care, reducing bloat reduces them all.&lt;/p&gt; &lt;p&gt;This post focuses on the “bloated container images” mistake that’s pretty easy to make when building Node.js application container images. Keep reading to learn about container image layers, how you can slim down a Node.js container image based on &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat’s Universal Base Images&lt;/a&gt; by over 70% (see Figure 1), and even more &lt;a href="https://developers.redhat.com/articles/2021/08/26/introduction-nodejs-reference-architecture-part-5-building-good-containers"&gt;best practices for building containers&lt;/a&gt;.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image1_16.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image1_16.png?itok=E9w80EPK" width="600" height="414" alt="A graph comparing the size of container images for a Node.js application depending on the build-strategy and base image used." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Image sizes resulting from a various build approaches using Red Hat's Universal Base Images for Node.js.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p class="Indent1"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;Note:&lt;/strong&gt; All of the examples and code used in this post can be found in &lt;a href="https://github.com/evanshortiss/nodejs-container-builds-example"&gt;this repository on GitHub&lt;/a&gt;. You’ll need Node.js 18 and either Docker or Podman installed to follow along. &lt;a href="https://podman-desktop.io/"&gt;Podman&lt;/a&gt; is the container engine used for the examples in this post. Substitute &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;code&gt;docker&lt;/code&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt; in place of &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;code&gt;podman&lt;/code&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt; in commands if you’re using Docker instead of Podman. &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Building a Node.js application container image&lt;/h2&gt; &lt;p&gt;The Node.js documentation provides a great overview of how to &lt;a href="https://nodejs.org/en/docs/guides/nodejs-docker-webapp/"&gt;configure a Containerfile (also known as a Dockerfile)&lt;/a&gt; for a basic Node.js application. Let’s use that as a template to create a container image for a Node.js application. This application will use the &lt;a href="https://fastify.io"&gt;Fastify framework&lt;/a&gt; and TypeScript, but it’s worth noting that this guide is applicable to any Node.js web framework.&lt;/p&gt; &lt;p&gt;To get started, generate a new Fastify project that uses TypeScript using the Fastify CLI:&lt;/p&gt; &lt;pre&gt; npx fastify-cli@5 generate --lang=ts nodejs-ts-basic # Change into the project directory and generate a package-lock.json cd nodejs-ts-basic npm i --package-lock-only&lt;/pre&gt; &lt;p&gt;&lt;br /&gt; Create a &lt;strong&gt;Containerfile&lt;/strong&gt; in the new &lt;code&gt;nodejs-ts-basic&lt;/code&gt; project directory with the following contents:&lt;/p&gt; &lt;pre&gt; FROM registry.access.redhat.com/ubi8/nodejs-18 WORKDIR /usr/src/app # Copy in package.json and package-lock.json COPY --chown=1001:1001 package*.json ./ # Install dependencies and devDependencies RUN npm ci # Copy in source code and other assets COPY --chown=1001:1001 . . # Compile the source TS into JS files RUN npm run build:ts # Configure fastify behaviour, and NODE_ENV ENV NODE_ENV=production ENV FASTIFY_PORT 8080 ENV FASTIFY_ADDRESS 0.0.0.0 EXPOSE 8080 # Set the fastify-cli binary as the entrypoint ENTRYPOINT [ "./node_modules/.bin/fastify" ] # Launch the container by passing these parameters to the entrypoint # These parameters can be overridden if you’d like CMD [ "start", "-l", "info", "dist/app.js" ]&lt;/pre&gt; &lt;p&gt;&lt;br /&gt; Next, create a &lt;code&gt;.dockerignore&lt;/code&gt; file in the root of the repository. This works similar to a &lt;code&gt;.gitignore&lt;/code&gt;, but is respected by tools like Podman and Docker. It’s used to avoid copying the specified files into a container image:&lt;/p&gt; &lt;pre&gt; # Change this as necessary for your own project(s) Containerfile* README.md dist node_modules test *.log .dockerignore .taprc .npmrc .env* &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; With these files in place, build a container image using the Podman (or Docker) CLI:&lt;/p&gt; &lt;pre&gt; podman build . -f Containerfile -t nodejs-ts-basic &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; The resulting container image is approximately 831 MB in size. That's 188 MB larger than the &lt;a href="https://catalog.redhat.com/software/containers/ubi8/nodejs-18/6278e5c078709f5277f26998?container-tabs=gti"&gt;UBI Node.js v18 base image&lt;/a&gt;! Investigate the size of files and folders by running the &lt;code&gt;du&lt;/code&gt; command inside the container:&lt;/p&gt; &lt;pre&gt; podman run --rm nodejs-ts-basic /bin/du -h -d 1 60K ./dist 28K ./src 190M ./node_modules 190M . &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; Clearly the &lt;code&gt;node_modules&lt;/code&gt; folder is causing bloat in the image, because both the &lt;code&gt;dependencies&lt;/code&gt; and the &lt;code&gt;devDependencies&lt;/code&gt; specified in the package.json were installed. &lt;/p&gt; &lt;h2&gt;Attempting to slim down the Node.js container image&lt;/h2&gt; &lt;p&gt;A seemingly obvious solution to this problem is to remove those &lt;code&gt;devDependencies&lt;/code&gt; from the image. Try that by adding &lt;code&gt;npm prune --omit=dev&lt;/code&gt; after the &lt;code&gt;npm run build:ts&lt;/code&gt; command in the &lt;strong&gt;Containerfile&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt; RUN npm run build:ts RUN npm prune --omit=dev &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; And building it:&lt;/p&gt; &lt;pre&gt; podman build . -f Containerfile -t nodejs-ts-basic-prune &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; The container image will be more lightweight, right? Let's check:&lt;/p&gt; &lt;pre&gt; podman images --format '{{.Size}} {{.Repository}}' | grep nodejs 831 MB localhost/nodejs-ts-basic 832 MB localhost/nodejs-ts-basic-prune &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; This new image is larger than the last one! This is because container images are composed of layers. Each layer stores changes compared to the prior layer it's based on. The &lt;code&gt;npm prune&lt;/code&gt; command removed the &lt;code&gt;devDependencies&lt;/code&gt; from the final container image layer (you can confirm using the &lt;code&gt;du&lt;/code&gt; command shown previously), but the layer containing them is still there. Confirm this using the &lt;code&gt;podman history localhost/nodejs-ts-basic-prune&lt;/code&gt; command, noting that the &lt;strong&gt;npm ci&lt;/strong&gt; layer is there, and is over 187 MB in size.&lt;/p&gt; &lt;h2&gt;Multi-stage builds to the rescue&lt;/h2&gt; &lt;p&gt;A great solution to this problem is to use a &lt;a href="https://docs.docker.com/build/building/multi-stage/"&gt;multi-stage build&lt;/a&gt;. Multi-stage builds perform some of the build steps in separate containers, and copy only what‘s needed into the final container image. This reduces the number of layers and overall size of the final container image.&lt;/p&gt; &lt;p&gt;This is an example of a multi-stage &lt;strong&gt;Containerfile&lt;/strong&gt; that can be used to slim down the Node.js container image:&lt;/p&gt; &lt;pre&gt; # First stage of the build is to install dependencies, and build from source FROM registry.access.redhat.com/ubi8/nodejs-18 as build WORKDIR /usr/src/app COPY --chown=1001:1001 package*.json ./ RUN npm ci COPY --chown=1001:1001 tsconfig*.json ./ COPY --chown=1001:1001 src src RUN npm run build:ts # Second stage of the build is to create a lighter container with just enough # required to run the application, i.e production deps and compiled js files FROM registry.access.redhat.com/ubi8/nodejs-18 WORKDIR /usr/src/app COPY --chown=1001:1001 --from=build /usr/src/app/package*.json/ . RUN npm ci --omit=dev COPY --chown=1001:1001 --from=build /usr/src/app/dist/ dist/ ENV NODE_ENV=production ENV FASTIFY_PORT 8080 ENV FASTIFY_ADDRESS 0.0.0.0 EXPOSE 8080 ENTRYPOINT [ "./node_modules/.bin/fastify" ] CMD [ "start", "-l", "info", "dist/app.js" ] &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; A summary of the two stages:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The first stage (&lt;strong&gt;build&lt;/strong&gt;) installs all dependencies, and compiles the TypeScript code.&lt;/li&gt; &lt;li&gt;The second stage copies the compiled code from the &lt;em&gt;build&lt;/em&gt; image and installs only the production dependencies to produce a deployable container image.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The initial build using this multi-stage &lt;strong&gt;Containerfile&lt;/strong&gt; will be slower, but subsequent builds benefit from cached layers and are only a second or two slower than the single stage build. &lt;/p&gt; &lt;p&gt;The multi-stage build results in a container image that's just 661 MB. That’s 20% smaller than the single stage image’s 831 MB. This isn’t bad—but you can do better.&lt;/p&gt; &lt;h2&gt;Going minimal&lt;/h2&gt; &lt;p&gt;There’s one last step you can take to really slim this Node.js container image down; and that’s using a minimal base image. A minimal base image contains as few tools and libraries as possible, which means they have a significantly smaller footprint. &lt;/p&gt; &lt;p&gt;Modifying the second stage of the multi-stage build to use &lt;a href="https://catalog.redhat.com/software/containers/rhel8/nodejs-18-minimal/627d1b055365187064a0c9db?container-tabs=gti"&gt;Red Hat's minimal Node.js v18 Universal Base Image&lt;/a&gt; reduces the final container image size to just 211 MB. All you need to do is change the &lt;code&gt;FROM&lt;/code&gt; statement to use the minimal image:&lt;/p&gt; &lt;pre&gt; FROM registry.access.redhat.com/ubi8/nodejs-18-minimal &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; That 211 MB container image is 75% smaller than the first 831 MB container image you built! Not only is the image smaller, but it also has a lower risk profile since it doesn’t contain tools that could be used for malicious purposes in the event of a security breach.&lt;/p&gt; &lt;h2&gt;Summary and next steps&lt;/h2&gt; &lt;p&gt;Use the following tips to improve your container images for any application runtime:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Use a trusted base image, such as &lt;a href="https://catalog.redhat.com/software/containers/search?q=ubi%20nodejs&amp;p=1"&gt;Red Hat’s Universal Base Image&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Don’t run as root. Using a Red Hat Universal Base Image takes care of this by default.&lt;/li&gt; &lt;li&gt;Use multi-stage builds to minimize container image layers.&lt;/li&gt; &lt;li&gt;Choose a minimal base image for the final stage in a multi-stage build.&lt;/li&gt; &lt;li&gt;Use a &lt;code&gt;.dockerignore&lt;/code&gt; file to keep unwanted files being copied into your container images.&lt;/li&gt; &lt;li&gt;Handle signals such as SIGINT and SIGTERM, or use &lt;a href="https://github.com/krallin/tini"&gt;tini&lt;/a&gt; or &lt;a href="https://github.com/Yelp/dumb-init"&gt;dumb-init&lt;/a&gt; to manage your process(es).&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Take a look at &lt;a href="https://developers.redhat.com/articles/2021/11/08/optimize-nodejs-images-ubi-8-nodejs-minimal-image"&gt;this post by Bethany Griggs&lt;/a&gt; when you’re ready to deploy your lean Node.js container images on the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/17/build-lean-nodejs-container-images-ubi-and-podman" title="Build lean Node.js container images with UBI and Podman"&gt;Build lean Node.js container images with UBI and Podman&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Evan Shortiss</dc:creator><dc:date>2023-05-17T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.37.0 released!</title><link rel="alternate" href="https://blog.kie.org/2023/05/kogito-1-37-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2023/05/kogito-1-37-0-released.html</id><updated>2023-05-16T10:42:39Z</updated><content type="html">We are glad to announce that the Kogito 1.37.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * ForEach state actions now can access the result of the previous action within the same iteration by using $WORKFLOW.prevActionResult. This allows action chaining in SWF loops. * When a task is not found, the GET rest API method now returns 404.  * New APIs to read and write a workflow definition from a yaml/json file * Sysout custom action now uses S4LJ rather than System.out. It optionally allows setting up the log level by adding it to the operation string.  * Service Discovery property expansion now supports the simplified format knative:&lt;namespace&gt;/&lt;serviceName&gt; For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.28.0 artifacts are available at the . A detailed changelog for 1.37.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title>The benefits of Fedora 38 long double transition in ppc64le</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/16/benefits-fedora-38-long-double-transition-ppc64le" /><author><name>Tulio Magno Quites Machado Filho</name></author><id>d06118ad-f5ac-4f38-b98b-6c5dbb3514a6</id><updated>2023-05-16T07:00:00Z</updated><published>2023-05-16T07:00:00Z</published><summary type="html">&lt;p&gt;Fedora 38 will have a new feature for ppc64le. Clang has begun using the IEEE 128-bit long double by default instead of the IBM 128-bit long double format. This allows Clang to behave the same way as GCC, which switched to IEEE 128-bit long double on ppc64le on &lt;a href="https://fedoraproject.org/wiki/Releases/36/ChangeSet#New_128-bit_IEEE_long_double_ABI_for_IBM_64-bit_POWER_LE"&gt;Fedora 36&lt;/a&gt;. This floating point format benefits from the hardware implementation available on IBM® Power9® processor-based servers and IBM® Power10™ processor-based servers.&lt;/p&gt; &lt;h2 id="background"&gt;Background of the floating point format&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/IBM_System/360_Model_85"&gt;IBM System/360 Model 85&lt;/a&gt; was released in 1968 and supported a 128-bit extended precision floating point format. A few decades later this format became known in the open source communities as IBM 128-bit long double or IBM double-double.&lt;/p&gt; &lt;p&gt;This floating point format provides a mantissa of 106 bits, 11 bits for the exponent and a signal bit. Meanwhile, its 64-bit floating point format provides a matissa of 53 bits, 11 bits as the exponent and a signal bit. According to the &lt;a href="https://www.ibm.com/docs/en/aix/7.1?topic=sepl-128-bit-long-double-floating-point-data-type"&gt;IBM® AIX® documentation&lt;/a&gt;, this data type can store numbers with more precision than the 64-bit data type, it does not store numbers of greater magnitude.&lt;/p&gt; &lt;p&gt;In 1985, the IEEE 754 Working Group for binary floating-point arithmetic established the &lt;a href="https://en.wikipedia.org/wiki/IEEE_754"&gt;IEEE Standard 754-1985&lt;/a&gt;, defining two binary floating point formats: a 32-bit (&lt;code&gt;binary32&lt;/code&gt;) and a 64-bit (&lt;code&gt;binary64&lt;/code&gt;). The C language was also in the process of standardization, requiring compilers to support at least three different binary floating point types called &lt;code&gt;float&lt;/code&gt;, &lt;code&gt;double&lt;/code&gt;, and &lt;code&gt;long double&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The ppc64le architecture on Linux adopted the &lt;code&gt;binary32&lt;/code&gt; format as &lt;code&gt;float&lt;/code&gt;, &lt;code&gt;binary64&lt;/code&gt; as &lt;code&gt;double&lt;/code&gt; and &lt;code&gt;ibm128&lt;/code&gt; as &lt;code&gt;long double&lt;/code&gt;, inheriting the same formats for the newer little endian architecture used on the older big endian ppc64.&lt;/p&gt; &lt;p&gt;In 2008, the IEEE Computer Society published the IEEE Std 754-2008, introducing an 128-bit binary floating point format (&lt;code&gt;binary128&lt;/code&gt;).&lt;/p&gt; &lt;table&gt;&lt;thead&gt;&lt;tr class="header"&gt;&lt;th&gt;Format&lt;/th&gt; &lt;th&gt;Signal bits&lt;/th&gt; &lt;th&gt;Exponent bits&lt;/th&gt; &lt;th&gt;Mantissa bits&lt;/th&gt; &lt;th&gt;Size (Bytes)&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr class="odd"&gt;&lt;td&gt;&lt;code&gt;binary32&lt;/code&gt;&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;24&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;/tr&gt;&lt;tr class="even"&gt;&lt;td&gt;&lt;code&gt;binary64&lt;/code&gt;&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;11&lt;/td&gt; &lt;td&gt;53&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;/tr&gt;&lt;tr class="odd"&gt;&lt;td&gt;&lt;code&gt;binary128&lt;/code&gt;&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;15&lt;/td&gt; &lt;td&gt;113&lt;/td&gt; &lt;td&gt;16&lt;/td&gt; &lt;/tr&gt;&lt;tr class="even"&gt;&lt;td&gt;&lt;code&gt;ibm128&lt;/code&gt;&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;11&lt;/td&gt; &lt;td&gt;106&lt;/td&gt; &lt;td&gt;16&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;In 2017, IBM introduced the Power9 processor with native hardware support for the &lt;code&gt;binary128&lt;/code&gt; format, leading the way to changing the default &lt;code&gt;long double&lt;/code&gt; type used in C and C++. However, this transition occurred only on ppc64le because it requires an IBM® Power8® processor or newer. The same transition on ppc64 is more complex because it may run on processors that do not support 128-bit registers from VSX or Altivec, requiring an emulation to happen on 64-bit general purpose registers as well as different rules for argument passing.&lt;/p&gt; &lt;h2&gt;How to transition to IEEE 128-bit long double&lt;/h2&gt; &lt;p&gt;In most cases, programs and libraries will not require any modifications. They must be rebuilt with the Clang provided with Fedora 38 to start using the IEEE 128-bit long double. While &lt;a href="https://www.nextplatform.com/2016/08/24/big-blue-aims-sky-power9/"&gt;IBM Power9&lt;/a&gt; introduced native hardware support for &lt;code&gt;binary128&lt;/code&gt;, a &lt;code&gt;long double&lt;/code&gt; based on this format also works on IBM Power8. The only difference is the performance improvement that newer processors provide.&lt;/p&gt; &lt;p&gt;Note that programs built with a previous version of Clang will continue to work using the IBM 128-bit long double.&lt;/p&gt; &lt;h3&gt;Adapting code to IEEE 128-bit long double&lt;/h3&gt; &lt;p&gt;There is a small group of programs that make assumptions about which &lt;code&gt;long double&lt;/code&gt; format ppc64le uses. In those cases, these programs have to be modified.&lt;/p&gt; &lt;p&gt;When rewriting this code, I suggest taking advantage of the features provided by the ISO C standard to write code that will be executed correctly on different processors and operating systems, regardless of the &lt;code&gt;long double&lt;/code&gt; format used by the C Library (e.g., using the &lt;a href="https://www.gnu.org/software/libc/manual/html_node/Floating-Point-Parameters.html#index-LDBL_005fMANT_005fDIG"&gt;macro LDBL_MANT_DIG&lt;/a&gt;) as follows:&lt;/p&gt; &lt;div class="sourceCode" id="cb1"&gt; &lt;pre class="c sourceCode"&gt; &lt;code class="sourceCode c"&gt;&lt;span class="pp"&gt;#include &lt;span class="im"&gt;&lt;float.h&gt; &lt;span class="pp"&gt;#if LDBL_MANT_DIG == 113 &lt;span class="co"&gt;/* Insert code for IEEE binary128 long double. */ &lt;span class="pp"&gt;#elif LDBL_MANT_DIG == 106 &lt;span class="co"&gt;/* Insert code for IBM 128-bit long double. */ &lt;span class="pp"&gt;#elif LDBL_MANT_DIG == 64 &lt;span class="co"&gt;/* Insert code for Intel 80-bit long double. */ &lt;span class="pp"&gt;#elif LDBL_MANT_DIG == 53 &lt;span class="co"&gt;/* Insert code for IEEE binary64 long double. */ &lt;span class="pp"&gt;#endif&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;h2&gt;You can still use IBM 128-bit long double&lt;/h2&gt; &lt;p&gt;It is also possible to continue using the IBM 128-bit long double with Clang on Fedora 38. When building the source code, ensure the parameters &lt;code&gt;-mabi=ibmlongdouble -mlong-double-128&lt;/code&gt; are passed to Clang as follows:&lt;/p&gt; &lt;div class="sourceCode" id="cb2"&gt; &lt;pre class="sh sourceCode"&gt; &lt;code class="sourceCode bash"&gt;$ clang -c -mabi=ibmlongdouble &lt;span class="at"&gt;-mlong-double-128 test.c -o test.o&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;p&gt;The same parameters also work on GCC.&lt;/p&gt; &lt;p&gt;C++ programs built with Clang must also get linked to libstdc++ because other libraries, such as libc++, do not support both &lt;code&gt;long double&lt;/code&gt; formats. The Fedora builds of Clang use libstdc++ by default, but if you would like to enforce the usage of libstdc++, use &lt;code&gt;-stdlib=libstdc++&lt;/code&gt; when calling &lt;code&gt;clang++&lt;/code&gt;, as follows:&lt;/p&gt; &lt;div class="sourceCode" id="cb3"&gt; &lt;pre class="sh sourceCode"&gt; &lt;code class="sourceCode bash"&gt;$ clang++ &lt;span class="at"&gt;-c -mabi=ibmlongdouble &lt;span class="at"&gt;-mlong-double-128 &lt;span class="at"&gt;-stdlib&lt;span class="op"&gt;=libstdc++ test.cc &lt;span class="at"&gt;-o test.o&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;h2&gt;Benefits of transitioning to IEEE 128-bit long double&lt;/h2&gt; &lt;p&gt;The transition to IEEE 128-bit long double on ppc64le will allow programs to compute numbers with greater magnitude and more precision without causing any performance regressions on IBM Power9 and newer processors. This transition is expected to help scientific and engineering programs as well as improve platform compatibility with well-established standards.&lt;/p&gt; &lt;p&gt;Feel free to comment below if you have questions or comments. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/16/benefits-fedora-38-long-double-transition-ppc64le" title="The benefits of Fedora 38 long double transition in ppc64le"&gt;The benefits of Fedora 38 long double transition in ppc64le&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Tulio Magno Quites Machado Filho</dc:creator><dc:date>2023-05-16T07:00:00Z</dc:date></entry><entry><title type="html">Q2 2023 RESTEasy Quarterly Releases</title><link rel="alternate" href="https://resteasy.dev/2023/05/16/resteasy-releases/" /><author><name /></author><id>https://resteasy.dev/2023/05/16/resteasy-releases/</id><updated>2023-05-16T04:11:11Z</updated><dc:creator /></entry></feed>
